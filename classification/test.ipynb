{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3 samples, 2 classes, 2 outputs\n",
    "y_true = [[1, 0], [1, 1], [0, 1]]\n",
    "y_pred = [[1, 1], [0, 0], [1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.6666666666666666, 0.4444444444444444, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(y_true, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = (1/2+0+1/2)/3 = 0.33\n",
    "# recall = (1/1+0+1/1)/3 = 0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, recall and f1 @ k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.6666666666666666)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_true = ['a','b','e'] = [1, 1, 0, 0, 1]\n",
    "#y_pred = ['b','c','a','e','d']\n",
    "\n",
    "# p@1: y_pred = [0, 0, 0, 0, 0]. p = 0\n",
    "# p@3: y_pred = [1, 1, 1]. p = 2/3\n",
    "# p@5: y_pred = [1, 1, 1, 1, 1]. p = 3/5\n",
    "\n",
    "y_true = [[1, 1, 0, 0, 1]]\n",
    "y_pred = [[1, 1, 1, 0, 0]]\n",
    "metrics.precision_score(y_true, y_pred, average='samples'), metrics.recall_score(y_true, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.3333333333333333)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[1, 1, 0, 0, 1]]\n",
    "y_pred = [[1, 0, 0, 0, 0]]\n",
    "metrics.precision_score(y_true, y_pred, average='samples'), metrics.recall_score(y_true, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 1, 0, 0, 1]]\n",
    "y_pred = [[1, 0, 0, 0, 0]]\n",
    "metrics.precision_score(y_true, y_pred, average='samples'), metrics.recall_score(y_true, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall_at_k_score(y_true, y_pred, at_k, labels=None, average=None, sample_weight=None):\n",
    "    if average not in (None, 'micro', 'macro', 'weighted', 'samples'):\n",
    "        raise ValueError(\"{0} average is not supported\".format(average))\n",
    "    y_pred = select_k_best(scores=y_pred, k_best=at_k, cast_as_indicator=True)\n",
    "    return metrics.recall_score(y_true, y_pred, labels=labels, average=average, sample_weight=sample_weight)\n",
    "\n",
    "def precision_at_k_score(y_true, y_pred, at_k, labels=None, average=None, sample_weight=None): \n",
    "    if average not in (None, 'micro', 'macro', 'weighted', 'samples'):\n",
    "        raise ValueError(\"{0} average is not supported\".format(average))\n",
    "    y_pred = select_k_best(scores=y_pred, k_best=at_k, cast_as_indicator=True)\n",
    "    return metrics.precision_score(y_true, y_pred, labels=labels, average=average, sample_weight=sample_weight)\n",
    "\n",
    "def select_k_best(scores, k_best, cast_as_indicator=False):\n",
    "    \"\"\"Select the k-best scores per sample.\n",
    "    :param scores: array, shape = [n_samples, n_labels] array / sparse matrix\n",
    "        Target scores, can either be probability estimates of the positive\n",
    "        class, confidence values, or binary decisions.\n",
    "    :param k_best: The number of best scores kept.\n",
    "    :param cast_as_indicator: return multilabel-indicator matrix instead of\n",
    "        matrix of scores\n",
    "    :return: array/matrix of the same format that input scores\n",
    "    \"\"\"\n",
    "    from sklearn.utils import check_array \n",
    "    from numpy import argpartition\n",
    "    from scipy import sparse\n",
    "    \n",
    "    scores = check_array(scores, accept_sparse='csr')\n",
    "\n",
    "    # scores_type = type_of_target(scores)\n",
    "    # if scores_type not in ('continuous-multioutput', 'multilabel-indicator'):\n",
    "    #     raise ValueError(\"{0} format is not supported\".format(scores_type))\n",
    "\n",
    "    best_scores = sparse.csr_matrix(scores, copy=True)\n",
    "    for i in range(best_scores.shape[0]):\n",
    "        # get the row slice per reference\n",
    "        row_array = best_scores.data[\n",
    "                    best_scores.indptr[i]: best_scores.indptr[i + 1]]\n",
    "        # only take the k last elements in the sorted indices,\n",
    "        # and set them to 0\n",
    "        row_array[argpartition(row_array, kth=-k_best)[:-k_best]] = 0\n",
    "    best_scores.eliminate_zeros()\n",
    "    if cast_as_indicator:\n",
    "        best_scores = best_scores.astype(bool).astype(int)\n",
    "        if sparse.issparse(scores):\n",
    "            return best_scores\n",
    "        return best_scores.todense()\n",
    "    if sparse.issparse(scores):\n",
    "        return best_scores\n",
    "    return best_scores.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[1, 1, 0, 1, 1, 0, 0]]\n",
    "y_pred = [[0.3, 0.5, 0, 0, 0, 0, 0]] # [[0, 1, 0, 0, 0]] @1\n",
    "y_pred = [[0.3, 0.5, 0, 0.9, 0, 0, 0]] # [[1, 1, 0, 0, 0]] @2\n",
    "precision_at_k_score(y_true, y_pred, at_k=3, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-expt",
   "language": "python",
   "name": "nlp-expt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
